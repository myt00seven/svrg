In the code from CIFAR wepage, the dataset is read as:

print(train_x.shape) 	(494021, 41)
print(train_y.shape) 	(494021,)
print(test_x.shape) 	(311029, 41)
print(test_y.shape) 	(311029,)

In the code for MNIST, the dataset is read as:


X_train
(50000, 1, 28, 28)
float32
y_train
(50000,)
X_val
(10000, 1, 28, 28)
y_val
(10000,)
X_test
(10000, 1, 28, 28)
y_test
(10000,)

Todo:
1) read the CIFAR dataset
2) put the origianl alexnet into work
3) modify the original alexnet, change LRN layer to BN layer 

1)
it seems now the CIFAR dataset is read. 

2)
compute the mean_file
the BN use different prediction for training and predicting

I just realized that we can't apply Alexnet to CIFAR. Since the structure of conv layer highly rely on the specifics of the input data. Changing the dimension of data also means change the structure of the cnn network and thus it is no more the same model.

===========================================================================

Ref: http://sachintalathi.com/?p=546

todo:
1) read the CFIAR dataste √
2) run the "naive" CNN model with original BN layer √
3) add data output to output val_loss and val_acc √
4) adjust the drawing python script accordingly 
5) add customized BN layer \ DBN to the model 
5.2) add control for main.py √
6) small scale, all models test (5 epoches maybe)
	-	from the obervation of first experiment, the loss_val is decreasing after 20 epochs. So I estimate the full scale will take up to 30-50 epochs to run.
7) estimate time and epoches for best result
8) run all 10 models for full scale experiment 
9) The MNIST and NI dataset is run with sgd instead of adagrad! Need to rerun them as well!
9.1) store the old results run with sgd
9.2) rerun MNIST with adagrad and update all the graphs and tables
9.3) rerun NI with adagrad and update all the graphs and tables

10) focus on abastract
















